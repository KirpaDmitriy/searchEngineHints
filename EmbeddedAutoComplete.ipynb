{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmbeddedAutoComplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Эксперименты с эмбеддингами</h1>"
      ],
      "metadata": {
        "id": "jfUrIzp4RJqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала приведу текст к чистому состоянию."
      ],
      "metadata": {
        "id": "84tZyX2wRVx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ' '.join(open('Пугачев.txt').readlines())\n",
        "\n",
        "alphabet = 'абвгдеёжзийклмнопртсухфцчшщьыьэюяabcdefghijklmnopqrstuvwxyz'\n",
        "stop_signs = '.!?'\n",
        "\n",
        "def looks_like_word(text):\n",
        "  ok = False\n",
        "  for letter in alphabet:\n",
        "    if letter in text.lower():\n",
        "      ok = True\n",
        "      break\n",
        "  return ok\n",
        "\n",
        "def stop_separator(text):\n",
        "  sep = ''\n",
        "  for stop in stop_signs:\n",
        "    if stop in text.lower():\n",
        "      sep = stop\n",
        "      break\n",
        "  return sep\n",
        "\n",
        "sentences = ['']\n",
        "for token in data.split():\n",
        "  if looks_like_word(token):\n",
        "    sign_inside = stop_separator(token)\n",
        "    if len(sign_inside) == 0:\n",
        "      sentences[-1] += token + ' '\n",
        "    else:\n",
        "      mas = token.split(sign_inside)\n",
        "      for elem in range(len(mas)):\n",
        "        sentences[-1] += mas[elem] + ' '\n",
        "        if elem + 1 < len(mas):\n",
        "          sentences.append('')\n",
        "  else:\n",
        "    if len(stop_separator(token)) != 0:\n",
        "      sentences.append('')\n",
        "\n",
        "\n",
        "clear_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "  clear_sentences.append('')\n",
        "  for symbol in sentence:\n",
        "    if symbol.isalpha() or symbol == ' ':\n",
        "      clear_sentences[-1] += symbol\n",
        "  clear_sentences[-1] = clear_sentences[-1].split()\n",
        "\n",
        "clear_sentences[20:30:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRJr5cxSFs13",
        "outputId": "a1f2611c-0961-4420-b6ea-c42c1074323d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Побег', 'кочующего', 'народа'],\n",
              " ['Его',\n",
              "  'течение',\n",
              "  'быстро',\n",
              "  'мутные',\n",
              "  'воды',\n",
              "  'наполнены',\n",
              "  'рыбою',\n",
              "  'всякого',\n",
              "  'рода',\n",
              "  'берега',\n",
              "  'большею',\n",
              "  'частию',\n",
              "  'глинистые',\n",
              "  'песчаные',\n",
              "  'и',\n",
              "  'безлесные',\n",
              "  'но',\n",
              "  'в',\n",
              "  'местах',\n",
              "  'поемных',\n",
              "  'удобные',\n",
              "  'для',\n",
              "  'скотоводства']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По имеющимся данным попытаюсь построить эмбеддинги (чем ближе слова будут в координатном пространстве, тем более подхоядщими по смыслу они будут, тем чаще идут вместе)."
      ],
      "metadata": {
        "id": "tkcNya-DRcjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "from multiprocessing import cpu_count\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(clear_sentences, min_count=0, workers=cpu_count())\n",
        "\n",
        "print(w2v_model.wv['любовь'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uteu2tB4GrDm",
        "outputId": "5f517df0-a3ac-4be1-af74-13c62af6ef6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.8512619e-03 -4.3107467e-03 -5.0928579e-03 -7.8272948e-04\n",
            " -3.3063896e-04 -4.5096874e-03 -2.9040745e-03 -2.0636597e-03\n",
            " -2.6016887e-03 -3.5684090e-03 -1.6257756e-03  6.2840199e-03\n",
            " -1.8738741e-03  3.1306907e-03 -6.2011200e-04 -1.6537684e-03\n",
            " -5.5663255e-03  4.0362594e-03  2.7452693e-03  3.6764906e-03\n",
            "  2.6353071e-03 -6.6098175e-04  4.2350036e-03 -3.6084035e-05\n",
            "  8.6874969e-04  3.3424343e-03  5.4005906e-03  6.7211138e-03\n",
            " -4.2289188e-03 -1.5309694e-03  4.9110330e-03  1.9595632e-03\n",
            "  3.4423696e-03  2.7240783e-03  7.1146311e-03  1.7185184e-03\n",
            " -1.4193208e-03 -1.6993393e-04  3.1732544e-03  2.8022041e-03\n",
            "  1.1188762e-03  2.3828281e-04  2.1661888e-03 -1.3110507e-03\n",
            "  3.7518109e-03 -8.8370702e-04  4.1199005e-03 -5.1272907e-03\n",
            "  3.0157086e-03  2.5700536e-03 -4.5978134e-03  6.9370517e-04\n",
            "  1.6258479e-04 -1.2268510e-03  2.8098153e-03 -4.9290241e-04\n",
            " -2.5097610e-04 -7.4694760e-04  2.9140597e-03  5.2233478e-03\n",
            " -6.6767554e-03  2.8783802e-04 -6.0377113e-04 -5.9251124e-03\n",
            "  2.7222012e-04 -9.3098264e-04 -4.8941025e-04  5.0119534e-03\n",
            " -3.5322469e-03 -6.1825737e-03  9.9959457e-03 -3.8114272e-03\n",
            "  6.5344889e-03  1.8540375e-03  1.0117883e-03  2.1330987e-03\n",
            " -1.7796282e-03 -9.3179615e-03 -1.1890622e-02 -2.6294081e-03\n",
            " -6.5133451e-03 -4.2326581e-03  2.9516642e-03  1.9187459e-03\n",
            " -1.2813078e-03  7.1166432e-04 -1.0792641e-03  4.9208291e-03\n",
            "  1.7080333e-03  2.6960953e-03 -4.2313500e-03  6.3308869e-03\n",
            " -3.6054095e-03 -4.8611704e-03  4.1679628e-03  1.7003227e-03\n",
            " -1.4994305e-03  1.5851991e-03  7.5124082e-04  2.4042197e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w2v_model.wv.most_similar('Бунт'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WzDAY79HUgF",
        "outputId": "0eb32022-8be3-468d-dbf6-374ac0f93106"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('петлю', 0.3383217453956604), ('казней', 0.333809494972229), ('подымаются', 0.33150309324264526), ('нашествием', 0.3243948519229889), ('баталиона', 0.3240799903869629), ('Надеемся', 0.3216485381126404), ('Царицын', 0.3101961016654968), ('приказу', 0.30966800451278687), ('Действия', 0.30502933263778687), ('смену', 0.3028341233730316)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w2v_model.wv.most_similar('татары'))"
      ],
      "metadata": {
        "id": "mfVobSWPKFx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ade63ce-b183-47da-ae27-5ce2b8bfef0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('нашему', 0.7033239006996155), ('спешить', 0.7007172107696533), ('самозванцу', 0.6941964626312256), ('силами', 0.6919719576835632), ('отрядом', 0.6898075938224792), ('станицу', 0.6849314570426941), ('деревни', 0.682374119758606), ('октября', 0.6753102540969849), ('пушек', 0.6743494868278503), ('губернатору', 0.6735434532165527)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получил набор синонимичных слов, они действительно могут идти где-то рядом в тексте или служить заменой исследуемому слову, но неясно, как это использовать в автодополнении запроса."
      ],
      "metadata": {
        "id": "WgAf15MpRvVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробую похожий подход, но в векторное пространство переведу не отдельные слова, а целые предлжения."
      ],
      "metadata": {
        "id": "RE1ZGcNuSCeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import doc2vec\n",
        "\n",
        "def tagged_document(list_of_ListOfWords):\n",
        "    for x, ListOfWords in enumerate(list_of_ListOfWords):\n",
        "        yield doc2vec.TaggedDocument(ListOfWords, [x])\n",
        "\n",
        "data_train = list(tagged_document(clear_sentences))\n",
        "\n",
        "d2v_model = doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
        "\n",
        "d2v_model.build_vocab(data_train)\n",
        "\n",
        "d2v_model.train(data_train, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
        "\n",
        "vec1 = d2v_model.infer_vector(['я', 'вас', 'любил'])\n",
        "print(vec1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3j3IbBdubIh",
        "outputId": "40388c82-a724-4746-c9a2-ee768f3fd3ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.0615301  -0.11173794 -0.02546872  0.06077589 -0.07444113 -0.00151369\n",
            "  0.01513805  0.08924188  0.00573151 -0.12549391 -0.06643118  0.01989817\n",
            " -0.12096556  0.0127247  -0.02978231 -0.05801844  0.04731642 -0.03068523\n",
            "  0.09391801 -0.05286277 -0.1015524  -0.0136481  -0.01362483 -0.02586907\n",
            "  0.16192424 -0.03509224 -0.01037985 -0.02852956  0.03948101  0.04571668\n",
            "  0.10232787  0.05417688  0.03177855  0.03089807  0.03671477 -0.02773746\n",
            "  0.12772171 -0.03518483  0.00227068  0.1880481 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_sentences = {}\n",
        "for sentence in clear_sentences:\n",
        "  embedded_sentences[' '.join(sentence)] = d2v_model.infer_vector(sentence)\n",
        "embedded_sentences['Побег кочующего народа']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00WUaYB-OlYz",
        "outputId": "b3753189-fff9-451e-f2e2-cba237acd820"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.05150026, -0.1248804 , -0.03152464,  0.08531298, -0.07933158,\n",
              "       -0.01972366,  0.03062134,  0.08424878,  0.03676644, -0.14537479,\n",
              "       -0.06926925,  0.03498233, -0.1225356 ,  0.00997425, -0.03887725,\n",
              "       -0.09231061,  0.03826494, -0.00603526,  0.12281232, -0.04735072,\n",
              "       -0.09486103, -0.02162586, -0.0151908 , -0.01326367,  0.16914855,\n",
              "       -0.0301776 , -0.00780203, -0.03970532,  0.04878021,  0.06405453,\n",
              "        0.07803874,  0.04559334,  0.02327166,  0.06015848,  0.04130342,\n",
              "       -0.01448507,  0.13810274, -0.04107934,  0.01790597,  0.18583952],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vec2 = d2v_model.infer_vector(['я', 'вас', 'любил'])\n",
        "print(np.linalg.norm(vec1 - vec2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RocmM--ezzvU",
        "outputId": "3025af1c-3639-4ae2-8553-148c623fbd6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0623445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec3 = d2v_model.infer_vector(['ночь', 'улица', 'фонарь', 'аптека'])\n",
        "print(np.linalg.norm(vec1 - vec3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0YfhYoCNQKv",
        "outputId": "41cd86c1-7a6b-4550-b961-4188efb339ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.12904842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чем более различны текста - тем больше норма расстояния между ними."
      ],
      "metadata": {
        "id": "g7xe4MWtNdmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы использовать полученные данные в автодополнении, для введённой фразы буду искать наиболее близкую к ней из исходгого набора текста."
      ],
      "metadata": {
        "id": "0b7IEVekSNqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_nearest_neighbors(sentence, k):\n",
        "  sentence = d2v_model.infer_vector(sentence.lower().split())\n",
        "  return list(zip(*sorted(list(map(lambda key: (np.linalg.norm(sentence - embedded_sentences[key]), key), embedded_sentences.keys())))))[1][:k]"
      ],
      "metadata": {
        "id": "LeSlURDuNlao"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_k_nearest_neighbors('Сгоревшие деревянные укрепления были заменены снеговыми', 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajfb38cDP1AB",
        "outputId": "c72bdc40-e89c-4bcd-e087-6d0cf99c9600"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Несколько четвертей муки и сухарей валялись на дворе',\n",
              " 'Перфильев перекрестясь простерся ниц и остался недвижим',\n",
              " 'Одни казачки были приняты',\n",
              " 'Деревни башкирские были пусты',\n",
              " 'Распоряжение Рейнсдорпа')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построение дало довольно странный результат. Видимо, влияет недостаток входных данных. Одако же для более крупных текстов данный подход слабо применим, так как в момент, когда пользователь делает запрос, нужно перебрать расстояния до всех имеющихся предложений. Это довольно долго."
      ],
      "metadata": {
        "id": "b6rSQ4fjUVID"
      }
    }
  ]
}